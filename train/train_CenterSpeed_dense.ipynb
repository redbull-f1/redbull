{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import wandb\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "# ë¡œì»¬ ëª¨ë“ˆ importë¡œ ë³€ê²½\n",
    "from models.resnet import *\n",
    "from models.CenterSpeed import *\n",
    "from dataset.CenterSpeed_dataset import *\n",
    "from models.losses import *\n",
    "from train import *\n",
    "\n",
    "\n",
    "%env \"WANDB_NOTEBOOK_NAME\" \"centerspeed.ipynb\"\n",
    "print(wandb.__version__)\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_to_pixel(x, y, image_size=[64,64], pixel_size=0.1):\n",
    "    pixel_x = int(x / pixel_size + image_size[0] / 2)\n",
    "    pixel_y = int(y / pixel_size + image_size[1] / 2)\n",
    "    return pixel_x, pixel_y\n",
    "\n",
    "# torchvision transformsë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •\n",
    "from torchvision import transforms\n",
    "\n",
    "# ë°ì´í„° ì¦ê°• ì„¤ì • (45ë„ íšŒì „ ë° 50% í™•ë¥ ë¡œ ì¢Œìš° ë°˜ì „) ê·¼ë° ì§€ê¸ˆ Noneìœ¼ë¡œ ì„¤ì • ë˜ì–´ìˆìŒ \n",
    "# transform = transforms.Compose([transforms.RandomRotation(45),\n",
    "#                                 transforms.RandomHorizontalFlip(0.5)])\n",
    "\n",
    "# ì¼ë‹¨ transformì„ Noneìœ¼ë¡œ ì„¤ì • (ì‚¬ìš©ì ì •ì˜ RandomRotation, RandomFlipì´ ì—†ìœ¼ë¯€ë¡œ)\n",
    "transform = None\n",
    "\n",
    "# ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì ˆëŒ€ê²½ë¡œë¡œ ìœ ì§€ (ì›ë³¸ ë°ì´í„°ê°€ ìˆëŠ” ê³³)\n",
    "set = CenterSpeedDataset('/home/harry/ros2_ws/src/TinyCenterSpeed/dataset/data/TinyCenterSpeed_dataset/data/CenterSpeedDataset', transform=transform, dense=True)\n",
    "# ì´ dataset í•¨ìˆ˜ì—ì„œ ë°˜í™˜í•˜ëŠ” ê°’ì´ input, gt_heatmap, data, dense_data, is_free ì´ë ‡ê²Œ 5ê°œì„\n",
    "\n",
    "set.seq_len = 2 # 2ê°œì˜ í”„ë ˆì„ ì…ë ¥\n",
    "# í•™ìŠµ ì‹œ ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„° ì„¤ì • ì´ê²Œ heatmap ë§Œë“¤ë•Œ ìŠ¤ì¼€ì¼ ëœ ì¢Œí‘œë¥¼ ì‚¬ìš©í•˜ëŠ”ê±´ë° ì„¼ì„œ ë°ì´í„°ì˜ ì˜¤ì°¨ê°€ ìƒê¸¸ ìˆ˜ ìˆìŒì„ ê³ ë ¤í•´ì„œ ë§Œë“ ê±°ì§€ \n",
    "# 1.0ì´ë©´ ì›ë³¸ í¬ê¸°, 0.9ë©´ 10% ì¶•ì†Œ\n",
    "set.sx = 0.9\n",
    "set.sy = 0.9\n",
    "set.change_image_size(64)\n",
    "set.change_pixel_size(0.1)\n",
    "\n",
    "# Fixed: Use absolute path for validation_set.csv\n",
    "# validation_set.csv íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì„ ë¡œë“œ\n",
    "independent_test_set = LidarDatasetSeqOD('/home/harry/ros2_ws/src/TinyCenterSpeed/dataset/data/TinyCenterSpeed_dataset/data/validation_set.csv')\n",
    "independent_test_set.seq_len = 2\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ì—ì„œ ìƒ˜í”Œ ë¶ˆëŸ¬ì™€ì„œ ë°ì´í„° êµ¬ì„± ìš”ì†Œ ì‹œê°í™” \n",
    "\n",
    "input, gt_heatmap, data, dense_data, is_free = set[1]  \n",
    "# inputì€ (3, 64, 64) í˜•íƒœë¡œ ë˜ì–´ìˆìŒ\n",
    "# gt_heatmapì€ (64, 64) í˜•íƒœë¡œ ë˜ì–´ìˆìŒ\n",
    "# dataëŠ” (x, y, yaw, vx, vy, yaw_rate) í˜•íƒœë¡œ ë˜ì–´ìˆìŒ\n",
    "# dense_dataëŠ” (64, 64, 3) í˜•íƒœë¡œ ë˜ì–´ìˆìŒ\n",
    "# is_freeëŠ” (1, 64, 64) í˜•íƒœë¡œ ë˜ì–´ìˆìŒ   0ì€ ì¥ì• ë¬¼, 1ì€ ììœ ê³µê°„\n",
    "\n",
    "print(f'Data: GT')\n",
    "# plt.imshow(input[1].numpy(), cmap='gray')\n",
    "plt.imshow(gt_heatmap)\n",
    "plt.colorbar() #ìƒ‰ìƒ ìŠ¤ì¼€ì¼ ë°”\n",
    "plt.show()\n",
    "\n",
    "labels = ['VX', 'VY', 'YAW']\n",
    "print(f'Dense data shape: {dense_data.shape}\\n')\n",
    "\n",
    "for i in range(dense_data.shape[2]):\n",
    "    plt.imshow(dense_data[:,:,i])\n",
    "    print(f'Data: {labels[i]}')\n",
    "    print(f'Maximum from data: {data[i+2].item()}')\n",
    "    # Extract and print the maximum value\n",
    "    max_value = np.max(dense_data[:,:,i].numpy().flatten())\n",
    "    min_value = np.min(dense_data[:,:,i].numpy().flatten())\n",
    "    print(f\"Maximum value in image slice {i}: {max_value}\")\n",
    "    print(f\"Minimum value in image slice {i}: {min_value}\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST THE MODEL\n",
    "# í•™ìŠµ ì „ ëª¨ë¸ í…ŒìŠ¤íŠ¸ -> ì…ì¶œë ¥ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ëŠ”ì§€ \n",
    "\n",
    "model = CenterSpeedDense() # ëª¨ë¸ ì´ˆê¸°í™”, ê°€ì¤‘ì¹˜ëŠ” ëœë¤\n",
    "model.eval() #í‰ê°€ ëª¨ë“œ(ë“œë¡­ì•„ì›ƒ(í•™ìŠµ í•  ë•Œ ë‰´ëŸ° ì¼ë¶€ë¥¼ ë¬´ì‘ìœ™ë¡œ êº¼ë²„ë¦¬ëŠ” ë²•), ë°°ì¹˜ ì •ê·œí™” ë¹„í™œì„±í™”)\n",
    "\n",
    "output = model(input.unsqueeze(0)) #(6, 64, 64) â†’ (1, 6, 64, 64) ë°°ì¹˜ ì°¨ì› ì¶”ê°€ \n",
    "#pytorch ëª¨ë¸ì˜ ì…ë ¥ì€ í•­ìƒ ì•„ë˜ í˜•íƒœë¥¼ ê¸°ëŒ€í•¨ \n",
    "# B, C, H, W = ë°°ì¹˜ í¬ê¸°. ì²´ë„ìˆ˜, ë†’ì´, ë„ˆë¹„\n",
    "\n",
    "plt.imshow(input[0])\n",
    "plt.show()\n",
    "for i in range(output.shape[1]): #output.shape[1]ì€ 4ì„ (ì±„ë„ ìˆ˜)\n",
    "    plt.imshow(output[0,i].detach().numpy()) #detach : ê·¸ë˜ë””ì–¸íŠ¸ ë¶„ë¦¬\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### new Loss funcction\n",
    "\n",
    "def dense_loss(output, gt_heatmap, gt_dense_data, is_free, alpha=0.7, decay=1): #ì•ŒíŒŒë¡œ ê°€ì¤‘ì¹˜ ì¡°ì ˆ(ìœ„ì¹˜ì™€ ì†ë„ yawì— ë¹„ì¤‘ ì„¤ì •)\n",
    "    print(f'Output Shape: {output.shape}')\n",
    "    print(f'GT Heatmap Shape: {gt_heatmap.shape}')\n",
    "    print(f'GT Dense Shape: {gt_dense_data.shape}')\n",
    "    # fig, ax = plt.subplots(1,3, figsize=(15,5))\n",
    "    # ax[0].imshow(gt_heatmap[0])\n",
    "    # ax[0].set_title('GT Heatmap')\n",
    "    # ax[1].imshow(gt_dense_data[:,:,:,0].squeeze())\n",
    "    # ax[1].set_title('GT Dense Data')\n",
    "    # ax[2].imshow(output[0,:,:,0].squeeze().detach().numpy())\n",
    "\n",
    "\n",
    "    loss = 0\n",
    "    batch_size = output.shape[0]\n",
    "\n",
    "    w = gt_heatmap # unit heatmap ì •ë‹µ íˆíŠ¸ë§µì„ ê·¸ëŒ€ë¡œ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš© \n",
    "\n",
    "    #outputì€ (1, 64, 64, 4) í˜•íƒœë¡œ ë˜ì–´ìˆìŒ\n",
    "    # 64*64 í”½ì…€ ëª¨ë‘ì— ëŒ€í•´ loss ê³„ì‚°\n",
    "    #output[:,:,:,0] â†’ (B, H, W) â†’ ì˜ˆì¸¡ëœ heatmap ê°’\n",
    "    #.unsqueeze(-1) â†’ (B, H, W, 1) â†’ ì°¨ì› ë§ì¶¤\n",
    "    loss += (alpha * (1+w)* (output[:,:,:,0].unsqueeze(-1) - gt_heatmap)**2).sum() #ìœ„ì¹˜  \n",
    "    loss += ((1-alpha) * (1+w)* (output[:,:,:,1:] - gt_dense_data)**2).sum() #vx, vy, yaw\n",
    "\n",
    "    return loss/ batch_size\n",
    "\n",
    "# print(f'Output Shape: {output.shape}')\n",
    "plt.imshow(output[0,0].detach().numpy())\n",
    "plt.show()\n",
    "loss = dense_loss(output.permute(0,2,3,1), gt_heatmap.unsqueeze(0).unsqueeze(-1), dense_data.unsqueeze(0), is_free)\n",
    "# print(f'Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_wandb = True  # WandB í™œì„±í™”\n",
    "save_code = True\n",
    "# Define the hyperparameters, logged in wandb\n",
    "\n",
    "#backbone = ì´ë¯¸ì§€ feature ì¶”ì¶œ \n",
    "#heatmap head = ê°ì²´ì˜ ì¤‘ì‹¬ì  ì˜ˆì¸¡í•˜ëŠ” ì¶œë ¥\n",
    "#Dense head = ì¤‘ì‹¬ì  ì™¸ì˜ ì •ë³´ ì˜ˆì¸¡ \n",
    "\n",
    "epochs = 15\n",
    "learning_rate = 5e-4\n",
    "learning_rate_hm = 0.005\n",
    "learning_rate_head = 0.005\n",
    "architecture = \"CenterSpeed: Hourglass Deep with Sigmoid, & Dropout, BatchNorm and Head with 2 frames, lower resolution: 64x64, pixelsize 0.1\"\n",
    "dataset = \"Transfer learning test\"\n",
    "optimizer = \"Adam\"\n",
    "batch_size = 32\n",
    "timer = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "run_name = \"CenterSpeed_\" + timer\n",
    "loss_used = \"CenterSpeedLossFreev2 with updated logic for free tracks on the dataset level\"\n",
    "\n",
    "#wandb configurations\n",
    "config = {\n",
    "    \"epochs\": epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"architecture\": architecture,\n",
    "    \"dataset\": dataset,\n",
    "    \"optimizer\": optimizer,\n",
    "    \"Loss-Function\": loss_used\n",
    "}\n",
    "if use_wandb:\n",
    "    #initialize wandb run\n",
    "    run = wandb.init(project=\"CenterSpeedLowRes\", config=config, name=run_name, save_code=save_code)#initialize wandb\n",
    "\n",
    "\n",
    "# ë°ì´í„° ì…‹ ë‚˜ëˆ„ëŠ” ë¶€ë¶„\n",
    "# valì€ í•™ìŠµ ì¤‘ê°„ ì¤‘ê°„ ë§¤ epoch ë§ˆë‹¤ í‰ê°€\n",
    "# testëŠ” í•™ìŠµì´ ëë‚œ í›„ í‰ê°€\n",
    "train_size = int(len(set) * 1)  # ì „ì²´ ë°ì´í„° ì…‹ ì¤‘ 100%ë¥¼ í›ˆë ¨ì— ì‚¬ìš©\n",
    "val_size = int(len(set) * 0)  # 0% for validation since this set is seperate!\n",
    "test_size = len(set) - (train_size + val_size)  # Remaining 5% for testing\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(set, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "training_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "testing_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "validation_loader = DataLoader(independent_test_set, batch_size=batch_size, shuffle= False)\n",
    "\n",
    "print(\"Size of Training Set: \", len(train_dataset))\n",
    "print(\"Size of Testing Set: \", len(test_dataset))\n",
    "print(\"Size of Validation Set: \", len(val_dataset))\n",
    "\n",
    "net = CenterSpeedDense(image_size=64)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr= learning_rate)\n",
    "print(\"Optimizer Initialized\")\n",
    "\n",
    "loss_fn = dense_loss\n",
    "print(\"Loss function initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_learning_rates(optimizer):\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        print(f\"Learning rate of layer {i}: {param_group['lr']}\")\n",
    "\n",
    "print_learning_rates(optimizer)\n",
    "\n",
    "net.train()\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(\"GRAD: \", name)\n",
    "    else:\n",
    "        print(\"NO GRAD: \", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output, display\n",
    "\n",
    "\n",
    "# def train_epoch_Centerspeed_dense(training_loader, net, optimizer, loss_fn, device = 'cpu', use_wandb=False, pdf=None):\n",
    "#     running_loss = 0.\n",
    "#     last_loss = 0.\n",
    "\n",
    "#     plt.ion()\n",
    "#     fig, ax = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "#     for i, data in enumerate(training_loader):\n",
    "#         # Every data instance is an input + label pair\n",
    "#         inputs, gts, data, dense_data, is_free = data\n",
    "#         inputs = inputs.to(device)\n",
    "#         gts = gts.to(device)\n",
    "#         data = data.to(device)\n",
    "#         # Zero your gradients for every batch!\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Make predictions for this batch\n",
    "#         output = net(inputs)\n",
    "#         # Compute the loss and its gradients\n",
    "\n",
    "#         loss = loss_fn(output.permute(0,2,3,1), gts.unsqueeze(-1), dense_data, is_free)\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Adjust learning weights\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Gather data and report\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#         last_loss = loss.item() # loss per batch\n",
    "\n",
    "#         ##Plot the input, output and ground truth in a interactive plot\n",
    "#         for a in ax:\n",
    "#             a.clear()\n",
    "#         ax[0].imshow(inputs[0,0])\n",
    "#         ax[0].set_title('Input')\n",
    "#         ax[1].imshow(output[0,0].detach().numpy())\n",
    "#         ax[1].set_title('Output')\n",
    "#         ax[2].imshow(gts[0])\n",
    "#         ax[2].set_title('Ground Truth')\n",
    "#         print(np.max(output[0,0].detach().numpy()))\n",
    "#         clear_output(wait=True)\n",
    "#         display(fig)\n",
    "\n",
    "\n",
    "#         print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "#         if use_wandb:\n",
    "#             wandb.log({\"batch_loss\": last_loss/len(inputs)})#log the average loss per batch\n",
    "\n",
    "#     plt.show()\n",
    "#     return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display\n",
    "\n",
    "def train_epoch_Centerspeed_dense(training_loader, net, optimizer, loss_fn, device = 'cpu', use_wandb=True, pdf=None):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    heatmap_loss_total = 0.\n",
    "    dense_loss_total = 0.\n",
    "    \n",
    "    plt.ion()\n",
    "    fig, ax = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, gts, data, dense_data, is_free = data\n",
    "        inputs = inputs.to(device)\n",
    "        gts = gts.to(device)\n",
    "        data = data.to(device)\n",
    "        dense_data = dense_data.to(device)  # ì´ ì¤„ ì¶”ê°€!\n",
    "        is_free = is_free.to(device)        # ì´ ì¤„ë„ ì¶”ê°€!\n",
    "        \n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        output = net(inputs)\n",
    "        \n",
    "        # ê°œë³„ ì†ì‹¤ ê³„ì‚° (ëª¨ë‹ˆí„°ë§ìš©)\n",
    "        alpha = 0.7\n",
    "        w = gts.unsqueeze(-1)\n",
    "        \n",
    "        # íˆíŠ¸ë§µ ì†ì‹¤ê³¼ dense ì†ì‹¤ ë¶„ë¦¬ ê³„ì‚°\n",
    "        heatmap_loss = (alpha * (1+w) * (output.permute(0,2,3,1)[:,:,:,0].unsqueeze(-1) - gts.unsqueeze(-1))**2).mean()\n",
    "        dense_loss_val = ((1-alpha) * (1+w) * (output.permute(0,2,3,1)[:,:,:,1:] - dense_data)**2).mean()\n",
    "        \n",
    "        # ì „ì²´ ì†ì‹¤ ê³„ì‚°\n",
    "        loss = loss_fn(output.permute(0,2,3,1), gts.unsqueeze(-1), dense_data, is_free)\n",
    "        loss.backward()\n",
    "\n",
    "        # ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ ê³„ì‚°\n",
    "        total_norm = 0\n",
    "        for p in net.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        heatmap_loss_total += heatmap_loss.item()\n",
    "        dense_loss_total += dense_loss_val.item()\n",
    "        last_loss = loss.item() # loss per batch\n",
    "\n",
    "        ##Plot the input, output and ground truth in a interactive plot\n",
    "        for a in ax:\n",
    "            a.clear()\n",
    "        ax[0].imshow(inputs[0,0].cpu())  # CPUë¡œ ì´ë™í•´ì„œ ì‹œê°í™”\n",
    "        ax[0].set_title('Input')\n",
    "        ax[1].imshow(output[0,0].detach().cpu().numpy())  # CPUë¡œ ì´ë™\n",
    "        ax[1].set_title('Output')\n",
    "        ax[2].imshow(gts[0].cpu())  # CPUë¡œ ì´ë™\n",
    "        ax[2].set_title('Ground Truth')\n",
    "        \n",
    "        output_max = np.max(output[0,0].detach().cpu().numpy())\n",
    "        print(f'Batch {i+1}: Loss={last_loss:.4f}, Heatmap={heatmap_loss.item():.4f}, Dense={dense_loss_val.item():.4f}, Output_Max={output_max:.4f}')\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "\n",
    "        if use_wandb:\n",
    "            # ë” ìì„¸í•œ ë©”íŠ¸ë¦­ ë¡œê¹…\n",
    "            wandb.log({\n",
    "                \"batch_loss\": last_loss,\n",
    "                \"batch_loss_per_sample\": last_loss/len(inputs),\n",
    "                \"heatmap_loss\": heatmap_loss.item(),\n",
    "                \"dense_loss\": dense_loss_val.item(),\n",
    "                \"loss_ratio_heatmap_to_dense\": heatmap_loss.item() / (dense_loss_val.item() + 1e-8),\n",
    "                \"gradient_norm\": total_norm,\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "                \"output_max_value\": output_max,\n",
    "                \"output_mean_value\": np.mean(output[0,0].detach().cpu().numpy()),\n",
    "                \"batch_number\": i,\n",
    "                \"samples_processed\": (i + 1) * len(inputs)\n",
    "            })\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # ì—í¬í¬ í‰ê·  ê³„ì‚°\n",
    "    avg_loss = running_loss / len(training_loader)\n",
    "    avg_heatmap_loss = heatmap_loss_total / len(training_loader)\n",
    "    avg_dense_loss = dense_loss_total / len(training_loader)\n",
    "    \n",
    "    return avg_loss, avg_heatmap_loss, avg_dense_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch_Centerspeed_dense(validation_loader, net, loss_fn, device='cpu'):\n",
    "    \"\"\"\n",
    "    Validation í•¨ìˆ˜ - ë§¤ ì—í¬í¬ë§ˆë‹¤ ê²€ì¦ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n",
    "    \"\"\"\n",
    "    net.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜ (dropout, batchnorm ë¹„í™œì„±í™”)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    heatmap_loss_total = 0.0\n",
    "    dense_loss_total = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½ & ì†ë„ í–¥ìƒ)\n",
    "        for i, data in enumerate(validation_loader):\n",
    "            inputs, gts, data_batch, dense_data, is_free = data\n",
    "            \n",
    "            # ëª¨ë“  í…ì„œë¥¼ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
    "            inputs = inputs.to(device)\n",
    "            gts = gts.to(device)\n",
    "            data_batch = data_batch.to(device)\n",
    "            dense_data = dense_data.to(device)\n",
    "            is_free = is_free.to(device)\n",
    "            \n",
    "            # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "            output = net(inputs)\n",
    "            \n",
    "            # ê°œë³„ ì†ì‹¤ ê³„ì‚°\n",
    "            alpha = 0.7\n",
    "            w = gts.unsqueeze(-1)\n",
    "            \n",
    "            heatmap_loss = (alpha * (1+w) * (output.permute(0,2,3,1)[:,:,:,0].unsqueeze(-1) - gts.unsqueeze(-1))**2).mean()\n",
    "            dense_loss_val = ((1-alpha) * (1+w) * (output.permute(0,2,3,1)[:,:,:,1:] - dense_data)**2).mean()\n",
    "            \n",
    "            # ì „ì²´ ì†ì‹¤ ê³„ì‚°\n",
    "            loss = loss_fn(output.permute(0,2,3,1), gts.unsqueeze(-1), dense_data, is_free)\n",
    "            \n",
    "            # í†µê³„ ëˆ„ì \n",
    "            running_loss += loss.item()\n",
    "            heatmap_loss_total += heatmap_loss.item()\n",
    "            dense_loss_total += dense_loss_val.item()\n",
    "            total_samples += len(inputs)\n",
    "    \n",
    "    # í‰ê·  ê³„ì‚°\n",
    "    avg_val_loss = running_loss / len(validation_loader)\n",
    "    avg_val_heatmap_loss = heatmap_loss_total / len(validation_loader)\n",
    "    avg_val_dense_loss = dense_loss_total / len(validation_loader)\n",
    "    \n",
    "    net.train()  # ë‹¤ì‹œ í›ˆë ¨ ëª¨ë“œë¡œ ì „í™˜\n",
    "    \n",
    "    return avg_val_loss, avg_val_heatmap_loss, avg_val_dense_loss, total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "#net, optimizer, loss_fn, training_loader, validation_loader = initialize(config)\n",
    "net.to(device)\n",
    "\n",
    "# EPOCHS = 10000\n",
    "EPOCHS = 5  # ë” ë§ì€ ì—í¬í¬ë¡œ validation íš¨ê³¼ í™•ì¸\n",
    "losses = []\n",
    "heatmap_losses = []\n",
    "dense_losses = []\n",
    "val_losses = []  # Validation loss ì¶”ì \n",
    "best_val_loss = float('inf')  # ìµœê³  ì„±ëŠ¥ ì¶”ì \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch: {epoch}')\n",
    "    net.train()\n",
    "    \n",
    "    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ (ì˜µì…˜)\n",
    "    if epoch > 0 and epoch % 5 == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.9  # 5 ì—í¬í¬ë§ˆë‹¤ í•™ìŠµë¥  10% ê°ì†Œ\n",
    "    \n",
    "    # í•™ìŠµ ì‹¤í–‰\n",
    "    avg_loss, avg_heatmap_loss, avg_dense_loss = train_epoch_Centerspeed_dense(\n",
    "        training_loader=training_loader, \n",
    "        net=net, \n",
    "        optimizer=optimizer,\n",
    "        loss_fn=loss_fn, \n",
    "        device=device, \n",
    "        use_wandb=True\n",
    "    )\n",
    "\n",
    "    losses.append(avg_loss)\n",
    "    heatmap_losses.append(avg_heatmap_loss)\n",
    "    dense_losses.append(avg_dense_loss)\n",
    "    \n",
    "    # ğŸ”¥ Validation ì‹¤í–‰ (ëª¨ë¸ íŒŒë¼ë¯¸í„°ëŠ” ìˆ˜ì • ì•ˆë¨!)\n",
    "    val_loss, val_heatmap_loss, val_dense_loss, val_samples = validate_epoch_Centerspeed_dense(\n",
    "        validation_loader=validation_loader,\n",
    "        net=net,\n",
    "        loss_fn=loss_fn,\n",
    "        device=device\n",
    "    )\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_path = f'./trained_models/best_model_epoch_{epoch}.pt'\n",
    "        torch.save(net.state_dict(), best_model_path)\n",
    "        print(f\"ğŸŒŸ New best model saved! Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # WandBì— ì—í¬í¬ ë‹¨ìœ„ ë©”íŠ¸ë¦­ ë¡œê¹…\n",
    "    if use_wandb:\n",
    "        # ëª¨ë¸ ê°€ì¤‘ì¹˜ í†µê³„\n",
    "        weight_stats = {}\n",
    "        for name, param in net.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                weight_stats[f\"weights/{name}_mean\"] = param.data.mean().item()\n",
    "                weight_stats[f\"weights/{name}_std\"] = param.data.std().item()\n",
    "                weight_stats[f\"weights/{name}_max\"] = param.data.max().item()\n",
    "                weight_stats[f\"weights/{name}_min\"] = param.data.min().item()\n",
    "        \n",
    "        # ì¢…í•© ë©”íŠ¸ë¦­ ë¡œê¹… (train + validation)\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": avg_loss,\n",
    "            \"train_heatmap_loss\": avg_heatmap_loss,\n",
    "            \"train_dense_loss\": avg_dense_loss,\n",
    "            \"val_loss\": val_loss,  # ğŸ”¥ ê²€ì¦ ì†ì‹¤ ì¶”ê°€\n",
    "            \"val_heatmap_loss\": val_heatmap_loss,\n",
    "            \"val_dense_loss\": val_dense_loss,\n",
    "            \"loss_improvement\": (losses[0] - avg_loss) / losses[0] if len(losses) > 1 else 0,\n",
    "            \"val_train_gap\": val_loss - avg_loss,  # ê³¼ì í•© ì§€í‘œ\n",
    "            \"current_learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            **weight_stats\n",
    "        })\n",
    "\n",
    "    print(f'Epoch {epoch} completed!')\n",
    "    print(f'Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "    print(f'Train Heatmap: {avg_heatmap_loss:.4f} | Val Heatmap: {val_heatmap_loss:.4f}')\n",
    "    print(f'Train Dense: {avg_dense_loss:.4f} | Val Dense: {val_dense_loss:.4f}')\n",
    "    print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    print(f'Val samples: {val_samples}')\n",
    "    \n",
    "    # ê³¼ì í•© ê²½ê³ \n",
    "    if val_loss > avg_loss * 1.5:\n",
    "        print(\"âš ï¸  Possible overfitting detected!\")\n",
    "\n",
    "    # save the model every epoch - ë¡œì»¬ ê²½ë¡œë¡œ ë³€ê²½\n",
    "    if epoch % 1 == 0:\n",
    "        model_path = f'./trained_models/epoch_{epoch}.pt'\n",
    "        torch.save(net.state_dict(), model_path)\n",
    "        if use_wandb:\n",
    "            wandb.save(model_path)\n",
    "        print(f\"Model saved at epoch {epoch}\")\n",
    "\n",
    "# ìµœì¢… ì°¨íŠ¸ ìƒì„± (train vs validation ë¹„êµ)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# ì „ì²´ ì†ì‹¤ ë¹„êµ\n",
    "axes[0,0].plot(losses, 'b-', label='Train Loss', linewidth=2)\n",
    "axes[0,0].plot(val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "axes[0,0].set_title('Training vs Validation Loss')\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Loss')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True)\n",
    "\n",
    "# íˆíŠ¸ë§µ vs Dense ì†ì‹¤\n",
    "axes[0,1].plot(heatmap_losses, 'r-', label='Heatmap Loss')\n",
    "axes[0,1].plot(dense_losses, 'g-', label='Dense Loss')\n",
    "axes[0,1].set_title('Loss Components (Training)')\n",
    "axes[0,1].set_xlabel('Epoch')\n",
    "axes[0,1].set_ylabel('Loss')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True)\n",
    "\n",
    "# ê³¼ì í•© ì§€í‘œ (Val - Train)\n",
    "if len(val_losses) > 0:\n",
    "    overfitting_gap = [val - train for val, train in zip(val_losses, losses)]\n",
    "    axes[0,2].plot(overfitting_gap, 'purple', label='Val - Train Gap')\n",
    "    axes[0,2].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[0,2].set_title('Overfitting Indicator')\n",
    "    axes[0,2].set_xlabel('Epoch')\n",
    "    axes[0,2].set_ylabel('Loss Gap')\n",
    "    axes[0,2].legend()\n",
    "    axes[0,2].grid(True)\n",
    "\n",
    "# ì†ì‹¤ ë¹„ìœ¨\n",
    "if len(heatmap_losses) > 0 and len(dense_losses) > 0:\n",
    "    loss_ratios = [h/(d+1e-8) for h, d in zip(heatmap_losses, dense_losses)]\n",
    "    axes[1,0].plot(loss_ratios, 'orange', label='Heatmap/Dense Ratio')\n",
    "    axes[1,0].set_title('Loss Ratio (Heatmap/Dense)')\n",
    "    axes[1,0].set_xlabel('Epoch')\n",
    "    axes[1,0].set_ylabel('Ratio')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True)\n",
    "\n",
    "# ì†ì‹¤ ê°œì„ ë¥ \n",
    "if len(losses) > 1:\n",
    "    train_improvements = [(losses[0] - loss) / losses[0] * 100 for loss in losses]\n",
    "    val_improvements = [(val_losses[0] - loss) / val_losses[0] * 100 for loss in val_losses] if len(val_losses) > 1 else []\n",
    "    \n",
    "    axes[1,1].plot(train_improvements, 'blue', label='Train Improvement %')\n",
    "    if val_improvements:\n",
    "        axes[1,1].plot(val_improvements, 'red', label='Val Improvement %')\n",
    "    axes[1,1].set_title('Training Progress (%)')\n",
    "    axes[1,1].set_xlabel('Epoch')\n",
    "    axes[1,1].set_ylabel('Improvement (%)')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True)\n",
    "\n",
    "# ìµœì¢… ì„±ëŠ¥ ìš”ì•½\n",
    "axes[1,2].text(0.1, 0.8, f\"Best Val Loss: {best_val_loss:.4f}\", fontsize=12, transform=axes[1,2].transAxes)\n",
    "axes[1,2].text(0.1, 0.7, f\"Final Train Loss: {losses[-1]:.4f}\", fontsize=12, transform=axes[1,2].transAxes)\n",
    "axes[1,2].text(0.1, 0.6, f\"Final Val Loss: {val_losses[-1]:.4f}\", fontsize=12, transform=axes[1,2].transAxes)\n",
    "if len(val_losses) > 0:\n",
    "    axes[1,2].text(0.1, 0.5, f\"Overfitting Gap: {val_losses[-1] - losses[-1]:.4f}\", fontsize=12, transform=axes[1,2].transAxes)\n",
    "axes[1,2].text(0.1, 0.4, f\"Total Epochs: {len(losses)}\", fontsize=12, transform=axes[1,2].transAxes)\n",
    "axes[1,2].set_title('Training Summary')\n",
    "axes[1,2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== Training Summary ===\")\n",
    "print(f\"Initial Train Loss: {losses[0]:.4f}\")\n",
    "print(f\"Final Train Loss: {losses[-1]:.4f}\")\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"Overfitting Gap: {val_losses[-1] - losses[-1]:.4f}\")\n",
    "print(f\"Total Improvement: {((losses[0] - losses[-1]) / losses[0] * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if param.grad is None:\n",
    "        print(name, param.data)\n",
    "    if param.grad is not None:\n",
    "        print(name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = next(iter(training_loader))[0]\n",
    "input = input.to(device)  # GPUë¡œ ì´ë™ ì¶”ê°€!\n",
    "\n",
    "#ëª¨ë¸ ì¶”ë¡  ì„±ëŠ¥\n",
    "import time\n",
    "start = time.perf_counter()\n",
    "output = net(input)\n",
    "end = time.perf_counter()\n",
    "\n",
    "\n",
    "print(f'Input Shape: {input.shape}')\n",
    "print(f'Output Shape: {output.shape}')\n",
    "\n",
    "\n",
    "input, gt_heatmap, data, dense_data, is_free = set[100]\n",
    "input = input.unsqueeze(0).to(device)  # GPUë¡œ ì´ë™ ì¶”ê°€!\n",
    "output = net(input)\n",
    "\n",
    "print(f'Elapsed time: {(end-start)*1000} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, gt_heatmap, data, dense_data, is_free = set[np.random.randint(0, len(set))]\n",
    "input = input.unsqueeze(0).to(device)  # GPUë¡œ ì´ë™ ì¶”ê°€!\n",
    "\n",
    "plt.imshow(input[0,0].cpu())  # ì‹œê°í™”ë¥¼ ìœ„í•´ CPUë¡œ ì´ë™\n",
    "plt.show()\n",
    "max_index = np.unravel_index(output[0,0].detach().cpu().numpy().argmax(), output[0,0].shape)\n",
    "output[0,0] = F.softmax(output[0,0])\n",
    "print(data)\n",
    "for i in range(output.shape[1]):\n",
    "    plt.imshow(output[0,i].detach().cpu().numpy())\n",
    "    plt.plot(max_index[1], max_index[0], 'ro')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    if i == 0:\n",
    "        print(f'Output: Heatmap')\n",
    "        continue\n",
    "\n",
    "    value_at_max = output[0,i].detach().cpu().numpy()[max_index]\n",
    "    max_value = np.max(output[0,i].detach().cpu().numpy())\n",
    "    min_value = np.min(output[0,i].detach().cpu().numpy())\n",
    "\n",
    "    print(f'GT: {data[1+i]} ')\n",
    "    print(f'Output: {value_at_max} ')\n",
    "    print(f'Maximum value in image slice {i}: {max_value}')\n",
    "    print(f'Minimum value in image slice {i}: {min_value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
