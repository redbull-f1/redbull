{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import wandb\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "# 로컬 모듈 import로 변경\n",
    "from models.resnet import *\n",
    "from models.CenterSpeed import *\n",
    "from dataset.CenterSpeed_dataset import *\n",
    "from models.losses import *\n",
    "from train import *\n",
    "\n",
    "\n",
    "%env \"WANDB_NOTEBOOK_NAME\" \"centerspeed.ipynb\"\n",
    "print(wandb.__version__)\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_to_pixel(x, y, image_size=[64,64], pixel_size=0.1):\n",
    "    pixel_x = int(x / pixel_size + image_size[0] / 2)\n",
    "    pixel_y = int(y / pixel_size + image_size[1] / 2)\n",
    "    return pixel_x, pixel_y\n",
    "\n",
    "# torchvision transforms를 사용하도록 수정\n",
    "from torchvision import transforms\n",
    "\n",
    "# 데이터 증강 설정 (45도 회전 및 50% 확률로 좌우 반전) 근데 지금 None으로 설정 되어있음 \n",
    "# transform = transforms.Compose([transforms.RandomRotation(45),\n",
    "#                                 transforms.RandomHorizontalFlip(0.5)])\n",
    "\n",
    "# 일단 transform을 None으로 설정 (사용자 정의 RandomRotation, RandomFlip이 없으므로)\n",
    "transform = None\n",
    "\n",
    "# 데이터셋 경로를 절대경로로 유지 (원본 데이터가 있는 곳)\n",
    "set = CenterSpeedDataset('/home/harry/ros2_ws/src/TinyCenterSpeed/dataset/data/TinyCenterSpeed_dataset/data/CenterSpeedDataset', transform=transform, dense=True)\n",
    "# 이 dataset 함수에서 반환하는 값이 input, gt_heatmap, data, dense_data, is_free 이렇게 5개임\n",
    "\n",
    "set.seq_len = 2 # 2개의 프레임 입력\n",
    "# 학습 시 스케일링 파라미터 설정 이게 heatmap 만들때 스케일 된 좌표를 사용하는건데 센서 데이터의 오차가 생길 수 있음을 고려해서 만든거지 \n",
    "# 1.0이면 원본 크기, 0.9면 10% 축소\n",
    "set.sx = 0.9\n",
    "set.sy = 0.9\n",
    "set.change_image_size(64)\n",
    "set.change_pixel_size(0.1)\n",
    "\n",
    "# Fixed: Use absolute path for validation_set.csv\n",
    "# validation_set.csv 파일의 절대 경로를 사용하여 데이터셋을 로드\n",
    "independent_test_set = LidarDatasetSeqOD('/home/harry/ros2_ws/src/TinyCenterSpeed/dataset/data/TinyCenterSpeed_dataset/data/validation_set.csv')\n",
    "independent_test_set.seq_len = 2\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋에서 샘플 불러와서 데이터 구성 요소 시각화 \n",
    "\n",
    "input, gt_heatmap, data, dense_data, is_free = set[1]  \n",
    "# input은 (3, 64, 64) 형태로 되어있음\n",
    "# gt_heatmap은 (64, 64) 형태로 되어있음\n",
    "# data는 (x, y, yaw, vx, vy, yaw_rate) 형태로 되어있음\n",
    "# dense_data는 (64, 64, 3) 형태로 되어있음\n",
    "# is_free는 (1, 64, 64) 형태로 되어있음   0은 장애물, 1은 자유공간\n",
    "\n",
    "print(f'Data: GT')\n",
    "# plt.imshow(input[1].numpy(), cmap='gray')\n",
    "plt.imshow(gt_heatmap)\n",
    "plt.colorbar() #색상 스케일 바\n",
    "plt.show()\n",
    "\n",
    "labels = ['VX', 'VY', 'YAW']\n",
    "print(f'Dense data shape: {dense_data.shape}\\n')\n",
    "\n",
    "for i in range(dense_data.shape[2]):\n",
    "    plt.imshow(dense_data[:,:,i])\n",
    "    print(f'Data: {labels[i]}')\n",
    "    print(f'Maximum from data: {data[i+2].item()}')\n",
    "    # Extract and print the maximum value\n",
    "    max_value = np.max(dense_data[:,:,i].numpy().flatten())\n",
    "    min_value = np.min(dense_data[:,:,i].numpy().flatten())\n",
    "    print(f\"Maximum value in image slice {i}: {max_value}\")\n",
    "    print(f\"Minimum value in image slice {i}: {min_value}\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST THE MODEL\n",
    "# 학습 전 모델 테스트 -> 입출력 정상적으로 작동하는지 \n",
    "\n",
    "model = CenterSpeedDense() # 모델 초기화, 가중치는 랜덤\n",
    "model.eval() #평가 모드(드롭아웃(학습 할 때 뉴런 일부를 무작윙로 꺼버리는 법), 배치 정규화 비활성화)\n",
    "\n",
    "output = model(input.unsqueeze(0)) #(6, 64, 64) → (1, 6, 64, 64) 배치 차원 추가 \n",
    "#pytorch 모델의 입력은 항상 아래 형태를 기대함 \n",
    "# B, C, H, W = 배치 크기. 체널수, 높이, 너비\n",
    "\n",
    "plt.imshow(input[0])\n",
    "plt.show()\n",
    "for i in range(output.shape[1]): #output.shape[1]은 4임 (채널 수)\n",
    "    plt.imshow(output[0,i].detach().numpy()) #detach : 그래디언트 분리\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### new Loss funcction\n",
    "\n",
    "def dense_loss(output, gt_heatmap, gt_dense_data, is_free, alpha=0.7, decay=1): #알파로 가중치 조절(위치와 속도 yaw에 비중 설정)\n",
    "    print(f'Output Shape: {output.shape}')\n",
    "    print(f'GT Heatmap Shape: {gt_heatmap.shape}')\n",
    "    print(f'GT Dense Shape: {gt_dense_data.shape}')\n",
    "    # fig, ax = plt.subplots(1,3, figsize=(15,5))\n",
    "    # ax[0].imshow(gt_heatmap[0])\n",
    "    # ax[0].set_title('GT Heatmap')\n",
    "    # ax[1].imshow(gt_dense_data[:,:,:,0].squeeze())\n",
    "    # ax[1].set_title('GT Dense Data')\n",
    "    # ax[2].imshow(output[0,:,:,0].squeeze().detach().numpy())\n",
    "\n",
    "\n",
    "    loss = 0\n",
    "    batch_size = output.shape[0]\n",
    "\n",
    "    w = gt_heatmap # unit heatmap 정답 히트맵을 그대로 가중치로 사용 \n",
    "\n",
    "    #output은 (1, 64, 64, 4) 형태로 되어있음\n",
    "    # 64*64 픽셀 모두에 대해 loss 계산\n",
    "    #output[:,:,:,0] → (B, H, W) → 예측된 heatmap 값\n",
    "    #.unsqueeze(-1) → (B, H, W, 1) → 차원 맞춤\n",
    "    loss += (alpha * (1+w)* (output[:,:,:,0].unsqueeze(-1) - gt_heatmap)**2).sum() #위치  \n",
    "    loss += ((1-alpha) * (1+w)* (output[:,:,:,1:] - gt_dense_data)**2).sum() #vx, vy, yaw\n",
    "\n",
    "    return loss/ batch_size\n",
    "\n",
    "# print(f'Output Shape: {output.shape}')\n",
    "plt.imshow(output[0,0].detach().numpy())\n",
    "plt.show()\n",
    "loss = dense_loss(output.permute(0,2,3,1), gt_heatmap.unsqueeze(0).unsqueeze(-1), dense_data.unsqueeze(0), is_free)\n",
    "# print(f'Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_wandb = True  # WandB 활성화\n",
    "save_code = True\n",
    "# Define the hyperparameters, logged in wandb\n",
    "\n",
    "#backbone = 이미지 feature 추출 \n",
    "#heatmap head = 객체의 중심점 예측하는 출력\n",
    "#Dense head = 중심점 외의 정보 예측 \n",
    "\n",
    "epochs = 15\n",
    "learning_rate = 5e-4\n",
    "learning_rate_hm = 0.005\n",
    "learning_rate_head = 0.005\n",
    "architecture = \"CenterSpeed: Hourglass Deep with Sigmoid, & Dropout, BatchNorm and Head with 2 frames, lower resolution: 64x64, pixelsize 0.1\"\n",
    "dataset = \"Transfer learning test\"\n",
    "optimizer = \"Adam\"\n",
    "batch_size = 32\n",
    "timer = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "run_name = \"CenterSpeed_\" + timer\n",
    "loss_used = \"CenterSpeedLossFreev2 with updated logic for free tracks on the dataset level\"\n",
    "\n",
    "#wandb configurations\n",
    "config = {\n",
    "    \"epochs\": epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"architecture\": architecture,\n",
    "    \"dataset\": dataset,\n",
    "    \"optimizer\": optimizer,\n",
    "    \"Loss-Function\": loss_used\n",
    "}\n",
    "if use_wandb:\n",
    "    #initialize wandb run\n",
    "    run = wandb.init(project=\"CenterSpeedLowRes\", config=config, name=run_name, save_code=save_code)#initialize wandb\n",
    "\n",
    "\n",
    "# 데이터 셋 나누는 부분\n",
    "# val은 학습 중간 중간 매 epoch 마다 평가\n",
    "# test는 학습이 끝난 후 평가\n",
    "train_size = int(len(set) * 1)  # 전체 데이터 셋 중 100%를 훈련에 사용\n",
    "val_size = int(len(set) * 0)  # 0% for validation since this set is seperate!\n",
    "test_size = len(set) - (train_size + val_size)  # Remaining 5% for testing\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(set, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "training_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "testing_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "validation_loader = DataLoader(independent_test_set, batch_size=batch_size, shuffle= False)\n",
    "\n",
    "print(\"Size of Training Set: \", len(train_dataset))\n",
    "print(\"Size of Testing Set: \", len(test_dataset))\n",
    "print(\"Size of Validation Set: \", len(val_dataset))\n",
    "\n",
    "net = CenterSpeedDense(image_size=64)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr= learning_rate)\n",
    "print(\"Optimizer Initialized\")\n",
    "\n",
    "loss_fn = dense_loss\n",
    "print(\"Loss function initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_learning_rates(optimizer):\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        print(f\"Learning rate of layer {i}: {param_group['lr']}\")\n",
    "\n",
    "print_learning_rates(optimizer)\n",
    "\n",
    "net.train()\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(\"GRAD: \", name)\n",
    "    else:\n",
    "        print(\"NO GRAD: \", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output, display\n",
    "\n",
    "\n",
    "# def train_epoch_Centerspeed_dense(training_loader, net, optimizer, loss_fn, device = 'cpu', use_wandb=False, pdf=None):\n",
    "#     running_loss = 0.\n",
    "#     last_loss = 0.\n",
    "\n",
    "#     plt.ion()\n",
    "#     fig, ax = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "#     for i, data in enumerate(training_loader):\n",
    "#         # Every data instance is an input + label pair\n",
    "#         inputs, gts, data, dense_data, is_free = data\n",
    "#         inputs = inputs.to(device)\n",
    "#         gts = gts.to(device)\n",
    "#         data = data.to(device)\n",
    "#         # Zero your gradients for every batch!\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Make predictions for this batch\n",
    "#         output = net(inputs)\n",
    "#         # Compute the loss and its gradients\n",
    "\n",
    "#         loss = loss_fn(output.permute(0,2,3,1), gts.unsqueeze(-1), dense_data, is_free)\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Adjust learning weights\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Gather data and report\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#         last_loss = loss.item() # loss per batch\n",
    "\n",
    "#         ##Plot the input, output and ground truth in a interactive plot\n",
    "#         for a in ax:\n",
    "#             a.clear()\n",
    "#         ax[0].imshow(inputs[0,0])\n",
    "#         ax[0].set_title('Input')\n",
    "#         ax[1].imshow(output[0,0].detach().numpy())\n",
    "#         ax[1].set_title('Output')\n",
    "#         ax[2].imshow(gts[0])\n",
    "#         ax[2].set_title('Ground Truth')\n",
    "#         print(np.max(output[0,0].detach().numpy()))\n",
    "#         clear_output(wait=True)\n",
    "#         display(fig)\n",
    "\n",
    "\n",
    "#         print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "#         if use_wandb:\n",
    "#             wandb.log({\"batch_loss\": last_loss/len(inputs)})#log the average loss per batch\n",
    "\n",
    "#     plt.show()\n",
    "#     return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display\n",
    "\n",
    "def train_epoch_Centerspeed_dense(training_loader, net, optimizer, loss_fn, device = 'cpu', use_wandb=True, pdf=None):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    heatmap_loss_total = 0.\n",
    "    dense_loss_total = 0.\n",
    "    \n",
    "    plt.ion()\n",
    "    fig, ax = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, gts, data, dense_data, is_free = data\n",
    "        inputs = inputs.to(device)\n",
    "        gts = gts.to(device)\n",
    "        data = data.to(device)\n",
    "        dense_data = dense_data.to(device)  # 이 줄 추가!\n",
    "        is_free = is_free.to(device)        # 이 줄도 추가!\n",
    "        \n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        output = net(inputs)\n",
    "        \n",
    "        # 개별 손실 계산 (모니터링용)\n",
    "        alpha = 0.7\n",
    "        w = gts.unsqueeze(-1)\n",
    "        \n",
    "        # 히트맵 손실과 dense 손실 분리 계산\n",
    "        heatmap_loss = (alpha * (1+w) * (output.permute(0,2,3,1)[:,:,:,0].unsqueeze(-1) - gts.unsqueeze(-1))**2).mean()\n",
    "        dense_loss_val = ((1-alpha) * (1+w) * (output.permute(0,2,3,1)[:,:,:,1:] - dense_data)**2).mean()\n",
    "        \n",
    "        # 전체 손실 계산\n",
    "        loss = loss_fn(output.permute(0,2,3,1), gts.unsqueeze(-1), dense_data, is_free)\n",
    "        loss.backward()\n",
    "\n",
    "        # 그래디언트 노름 계산\n",
    "        total_norm = 0\n",
    "        for p in net.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        heatmap_loss_total += heatmap_loss.item()\n",
    "        dense_loss_total += dense_loss_val.item()\n",
    "        last_loss = loss.item() # loss per batch\n",
    "\n",
    "        ##Plot the input, output and ground truth in a interactive plot\n",
    "        for a in ax:\n",
    "            a.clear()\n",
    "        ax[0].imshow(inputs[0,0].cpu())  # CPU로 이동해서 시각화\n",
    "        ax[0].set_title('Input')\n",
    "        ax[1].imshow(output[0,0].detach().cpu().numpy())  # CPU로 이동\n",
    "        ax[1].set_title('Output')\n",
    "        ax[2].imshow(gts[0].cpu())  # CPU로 이동\n",
    "        ax[2].set_title('Ground Truth')\n",
    "        \n",
    "        output_max = np.max(output[0,0].detach().cpu().numpy())\n",
    "        print(f'Batch {i+1}: Loss={last_loss:.4f}, Heatmap={heatmap_loss.item():.4f}, Dense={dense_loss_val.item():.4f}, Output_Max={output_max:.4f}')\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "\n",
    "        if use_wandb:\n",
    "            # 더 자세한 메트릭 로깅\n",
    "            wandb.log({\n",
    "                \"batch_loss\": last_loss,\n",
    "                \"batch_loss_per_sample\": last_loss/len(inputs),\n",
    "                \"heatmap_loss\": heatmap_loss.item(),\n",
    "                \"dense_loss\": dense_loss_val.item(),\n",
    "                \"loss_ratio_heatmap_to_dense\": heatmap_loss.item() / (dense_loss_val.item() + 1e-8),\n",
    "                \"gradient_norm\": total_norm,\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "                \"output_max_value\": output_max,\n",
    "                \"output_mean_value\": np.mean(output[0,0].detach().cpu().numpy()),\n",
    "                \"batch_number\": i,\n",
    "                \"samples_processed\": (i + 1) * len(inputs)\n",
    "            })\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # 에포크 평균 계산\n",
    "    avg_loss = running_loss / len(training_loader)\n",
    "    avg_heatmap_loss = heatmap_loss_total / len(training_loader)\n",
    "    avg_dense_loss = dense_loss_total / len(training_loader)\n",
    "    \n",
    "    return avg_loss, avg_heatmap_loss, avg_dense_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch_Centerspeed_dense(validation_loader, net, loss_fn, device='cpu'):\n",
    "    \"\"\"\n",
    "    Validation 함수 - 매 에포크마다 검증 데이터셋으로 모델 성능 평가\n",
    "    \"\"\"\n",
    "    net.eval()  # 평가 모드로 전환 (dropout, batchnorm 비활성화)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    heatmap_loss_total = 0.0\n",
    "    dense_loss_total = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화 (메모리 절약 & 속도 향상)\n",
    "        for i, data in enumerate(validation_loader):\n",
    "            inputs, gts, data_batch, dense_data, is_free = data\n",
    "            \n",
    "            # 모든 텐서를 동일한 디바이스로 이동\n",
    "            inputs = inputs.to(device)\n",
    "            gts = gts.to(device)\n",
    "            data_batch = data_batch.to(device)\n",
    "            dense_data = dense_data.to(device)\n",
    "            is_free = is_free.to(device)\n",
    "            \n",
    "            # 예측 수행\n",
    "            output = net(inputs)\n",
    "            \n",
    "            # 개별 손실 계산\n",
    "            alpha = 0.7\n",
    "            w = gts.unsqueeze(-1)\n",
    "            \n",
    "            heatmap_loss = (alpha * (1+w) * (output.permute(0,2,3,1)[:,:,:,0].unsqueeze(-1) - gts.unsqueeze(-1))**2).mean()\n",
    "            dense_loss_val = ((1-alpha) * (1+w) * (output.permute(0,2,3,1)[:,:,:,1:] - dense_data)**2).mean()\n",
    "            \n",
    "            # 전체 손실 계산\n",
    "            loss = loss_fn(output.permute(0,2,3,1), gts.unsqueeze(-1), dense_data, is_free)\n",
    "            \n",
    "            # 통계 누적\n",
    "            running_loss += loss.item()\n",
    "            heatmap_loss_total += heatmap_loss.item()\n",
    "            dense_loss_total += dense_loss_val.item()\n",
    "            total_samples += len(inputs)\n",
    "    \n",
    "    # 평균 계산\n",
    "    avg_val_loss = running_loss / len(validation_loader)\n",
    "    avg_val_heatmap_loss = heatmap_loss_total / len(validation_loader)\n",
    "    avg_val_dense_loss = dense_loss_total / len(validation_loader)\n",
    "    \n",
    "    net.train()  # 다시 훈련 모드로 전환\n",
    "    \n",
    "    return avg_val_loss, avg_val_heatmap_loss, avg_val_dense_loss, total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "#net, optimizer, loss_fn, training_loader, validation_loader = initialize(config)\n",
    "net.to(device)\n",
    "\n",
    "# EPOCHS = 10000\n",
    "EPOCHS = 5  # 더 많은 에포크로 validation 효과 확인\n",
    "losses = []\n",
    "heatmap_losses = []\n",
    "dense_losses = []\n",
    "val_losses = []  # Validation loss 추적\n",
    "best_val_loss = float('inf')  # 최고 성능 추적\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch: {epoch}')\n",
    "    net.train()\n",
    "    \n",
    "    # 학습률 스케줄링 (옵션)\n",
    "    if epoch > 0 and epoch % 5 == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.9  # 5 에포크마다 학습률 10% 감소\n",
    "    \n",
    "    # 학습 실행\n",
    "    avg_loss, avg_heatmap_loss, avg_dense_loss = train_epoch_Centerspeed_dense(\n",
    "        training_loader=training_loader, \n",
    "        net=net, \n",
    "        optimizer=optimizer,\n",
    "        loss_fn=loss_fn, \n",
    "        device=device, \n",
    "        use_wandb=True\n",
    "    )\n",
    "\n",
    "    losses.append(avg_loss)\n",
    "    heatmap_losses.append(avg_heatmap_loss)\n",
    "    dense_losses.append(avg_dense_loss)\n",
    "    \n",
    "    # 🔥 Validation 실행 (모델 파라미터는 수정 안됨!)\n",
    "    val_loss, val_heatmap_loss, val_dense_loss, val_samples = validate_epoch_Centerspeed_dense(\n",
    "        validation_loader=validation_loader,\n",
    "        net=net,\n",
    "        loss_fn=loss_fn,\n",
    "        device=device\n",
    "    )\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 최고 성능 모델 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_path = f'./trained_models/best_model_epoch_{epoch}.pt'\n",
    "        torch.save(net.state_dict(), best_model_path)\n",
    "        print(f\"🌟 New best model saved! Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # WandB에 에포크 단위 메트릭 로깅\n",
    "    if use_wandb:\n",
    "        # 모델 가중치 통계\n",
    "        weight_stats = {}\n",
    "        for name, param in net.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                weight_stats[f\"weights/{name}_mean\"] = param.data.mean().item()\n",
    "                weight_stats[f\"weights/{name}_std\"] = param.data.std().item()\n",
    "                weight_stats[f\"weights/{name}_max\"] = param.data.max().item()\n",
    "                weight_stats[f\"weights/{name}_min\"] = param.data.min().item()\n",
    "        \n",
    "        # 종합 메트릭 로깅 (train + validation)\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": avg_loss,\n",
    "            \"train_heatmap_loss\": avg_heatmap_loss,\n",
    "            \"train_dense_loss\": avg_dense_loss,\n",
    "            \"val_loss\": val_loss,  # 🔥 검증 손실 추가\n",
    "            \"val_heatmap_loss\": val_heatmap_loss,\n",
    "            \"val_dense_loss\": val_dense_loss,\n",
    "            \"loss_improvement\": (losses[0] - avg_loss) / losses[0] if len(losses) > 1 else 0,\n",
    "            \"val_train_gap\": val_loss - avg_loss,  # 과적합 지표\n",
    "            \"current_learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            **weight_stats\n",
    "        })\n",
    "\n",
    "    print(f'Epoch {epoch} completed!')\n",
    "    print(f'Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "    print(f'Train Heatmap: {avg_heatmap_loss:.4f} | Val Heatmap: {val_heatmap_loss:.4f}')\n",
    "    print(f'Train Dense: {avg_dense_loss:.4f} | Val Dense: {val_dense_loss:.4f}')\n",
    "    print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    print(f'Val samples: {val_samples}')\n",
    "    \n",
    "    # 과적합 경고\n",
    "    if val_loss > avg_loss * 1.5:\n",
    "        print(\"⚠️  Possible overfitting detected!\")\n",
    "\n",
    "    # save the model every epoch - 로컬 경로로 변경\n",
    "    if epoch % 1 == 0:\n",
    "        model_path = f'./trained_models/epoch_{epoch}.pt'\n",
    "        torch.save(net.state_dict(), model_path)\n",
    "        if use_wandb:\n",
    "            wandb.save(model_path)\n",
    "        print(f\"Model saved at epoch {epoch}\")\n",
    "\n",
    "# 최종 차트 생성 (train vs validation 비교)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 전체 손실 비교\n",
    "axes[0,0].plot(losses, 'b-', label='Train Loss', linewidth=2)\n",
    "axes[0,0].plot(val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "axes[0,0].set_title('Training vs Validation Loss')\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Loss')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True)\n",
    "\n",
    "# 히트맵 vs Dense 손실\n",
    "axes[0,1].plot(heatmap_losses, 'r-', label='Heatmap Loss')\n",
    "axes[0,1].plot(dense_losses, 'g-', label='Dense Loss')\n",
    "axes[0,1].set_title('Loss Components (Training)')\n",
    "axes[0,1].set_xlabel('Epoch')\n",
    "axes[0,1].set_ylabel('Loss')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True)\n",
    "\n",
    "# 과적합 지표 (Val - Train)\n",
    "if len(val_losses) > 0:\n",
    "    overfitting_gap = [val - train for val, train in zip(val_losses, losses)]\n",
    "    axes[0,2].plot(overfitting_gap, 'purple', label='Val - Train Gap')\n",
    "    axes[0,2].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[0,2].set_title('Overfitting Indicator')\n",
    "    axes[0,2].set_xlabel('Epoch')\n",
    "    axes[0,2].set_ylabel('Loss Gap')\n",
    "    axes[0,2].legend()\n",
    "    axes[0,2].grid(True)\n",
    "\n",
    "# 손실 비율\n",
    "if len(heatmap_losses) > 0 and len(dense_losses) > 0:\n",
    "    loss_ratios = [h/(d+1e-8) for h, d in zip(heatmap_losses, dense_losses)]\n",
    "    axes[1,0].plot(loss_ratios, 'orange', label='Heatmap/Dense Ratio')\n",
    "    axes[1,0].set_title('Loss Ratio (Heatmap/Dense)')\n",
    "    axes[1,0].set_xlabel('Epoch')\n",
    "    axes[1,0].set_ylabel('Ratio')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True)\n",
    "\n",
    "# 손실 개선률\n",
    "if len(losses) > 1:\n",
    "    train_improvements = [(losses[0] - loss) / losses[0] * 100 for loss in losses]\n",
    "    val_improvements = [(val_losses[0] - loss) / val_losses[0] * 100 for loss in val_losses] if len(val_losses) > 1 else []\n",
    "    \n",
    "    axes[1,1].plot(train_improvements, 'blue', label='Train Improvement %')\n",
    "    if val_improvements:\n",
    "        axes[1,1].plot(val_improvements, 'red', label='Val Improvement %')\n",
    "    axes[1,1].set_title('Training Progress (%)')\n",
    "    axes[1,1].set_xlabel('Epoch')\n",
    "    axes[1,1].set_ylabel('Improvement (%)')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True)\n",
    "\n",
    "# 최종 성능 요약\n",
    "axes[1,2].text(0.1, 0.8, f\"Best Val Loss: {best_val_loss:.4f}\", fontsize=12, transform=axes[1,2].transAxes)\n",
    "axes[1,2].text(0.1, 0.7, f\"Final Train Loss: {losses[-1]:.4f}\", fontsize=12, transform=axes[1,2].transAxes)\n",
    "axes[1,2].text(0.1, 0.6, f\"Final Val Loss: {val_losses[-1]:.4f}\", fontsize=12, transform=axes[1,2].transAxes)\n",
    "if len(val_losses) > 0:\n",
    "    axes[1,2].text(0.1, 0.5, f\"Overfitting Gap: {val_losses[-1] - losses[-1]:.4f}\", fontsize=12, transform=axes[1,2].transAxes)\n",
    "axes[1,2].text(0.1, 0.4, f\"Total Epochs: {len(losses)}\", fontsize=12, transform=axes[1,2].transAxes)\n",
    "axes[1,2].set_title('Training Summary')\n",
    "axes[1,2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== Training Summary ===\")\n",
    "print(f\"Initial Train Loss: {losses[0]:.4f}\")\n",
    "print(f\"Final Train Loss: {losses[-1]:.4f}\")\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"Overfitting Gap: {val_losses[-1] - losses[-1]:.4f}\")\n",
    "print(f\"Total Improvement: {((losses[0] - losses[-1]) / losses[0] * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if param.grad is None:\n",
    "        print(name, param.data)\n",
    "    if param.grad is not None:\n",
    "        print(name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = next(iter(training_loader))[0]\n",
    "input = input.to(device)  # GPU로 이동 추가!\n",
    "\n",
    "#모델 추론 성능\n",
    "import time\n",
    "start = time.perf_counter()\n",
    "output = net(input)\n",
    "end = time.perf_counter()\n",
    "\n",
    "\n",
    "print(f'Input Shape: {input.shape}')\n",
    "print(f'Output Shape: {output.shape}')\n",
    "\n",
    "\n",
    "input, gt_heatmap, data, dense_data, is_free = set[100]\n",
    "input = input.unsqueeze(0).to(device)  # GPU로 이동 추가!\n",
    "output = net(input)\n",
    "\n",
    "print(f'Elapsed time: {(end-start)*1000} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, gt_heatmap, data, dense_data, is_free = set[np.random.randint(0, len(set))]\n",
    "input = input.unsqueeze(0).to(device)  # GPU로 이동 추가!\n",
    "\n",
    "plt.imshow(input[0,0].cpu())  # 시각화를 위해 CPU로 이동\n",
    "plt.show()\n",
    "max_index = np.unravel_index(output[0,0].detach().cpu().numpy().argmax(), output[0,0].shape)\n",
    "output[0,0] = F.softmax(output[0,0])\n",
    "print(data)\n",
    "for i in range(output.shape[1]):\n",
    "    plt.imshow(output[0,i].detach().cpu().numpy())\n",
    "    plt.plot(max_index[1], max_index[0], 'ro')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    if i == 0:\n",
    "        print(f'Output: Heatmap')\n",
    "        continue\n",
    "\n",
    "    value_at_max = output[0,i].detach().cpu().numpy()[max_index]\n",
    "    max_value = np.max(output[0,i].detach().cpu().numpy())\n",
    "    min_value = np.min(output[0,i].detach().cpu().numpy())\n",
    "\n",
    "    print(f'GT: {data[1+i]} ')\n",
    "    print(f'Output: {value_at_max} ')\n",
    "    print(f'Maximum value in image slice {i}: {max_value}')\n",
    "    print(f'Minimum value in image slice {i}: {min_value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
